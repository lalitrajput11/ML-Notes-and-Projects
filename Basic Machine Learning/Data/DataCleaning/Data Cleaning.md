

**Primary Data Sources** 

The data sources which provide primary data are known as primary data sources, and information gathered directly from first-hand experience is referred to as preliminary data. This is the information you collect for the aim of a particular research endeavour.

Primary data gathering is a straightforward method suited to a company’s particular requirements. It’s a time-consuming procedure, but it provides valuable first-hand knowledge in many business situations.

E.g., in Census data collected by the government, Stock prices are taken from the stock market.

---
**Secondary Data Sources**

These data sources provide secondary data. Secondary data has previously been gathered for another reason but is relevant to your investigation. Additionally, the data is collected by someone other than the team who needs the data.

Secondhand information is referred to as secondary data. It is not the first time it has been used, and that’s why it’s referred to as secondary.

Secondary data sources contribute to the interpretation and analysis of main data. They may describe primary materials in-depth and frequently utilize them to promote a certain thesis or point of view.

---
## Identifying and Gathering Data

The first step in identifying data is deciding what information needs to be gathered, which is decided by the aim we want to achieve. After we have identified the data, we will need to choose the sources from which we shall extract the essential information and create a data collecting strategy.

1. **Databases**

Databases, the web, social media, interactive platforms, sensor devices, data exchanges, surveys, and observation studies are some of the data sources we may be using. Data from diverse data sources is recognized and acquired, then merged using a range of tools and methodologies to create a unified interface for querying and manipulating data. The data we identify, the source of that data, and the methods we use to collect it all have quality, security, and privacy concerns that must be considered at this time.

2.  **Relational databases**, 

such as SQL Server, Oracle, MySQL, and IBM DB2, are used to store data in an organized manner in these systems


4. **APIs**

APIs, or Application Program Interfaces, and Web Services are provided by many data providers and websites, allowing various users or programmes to communicate with and access data for processing or analysis. APIs and Web Services often listen for incoming requests from users or applications, which might be in the form of web requests or network requests, and return data in plain text, XML, HTML, JSON, or media files.

5. **Web Scraping**

Web scraping is a technique for obtaining meaningful data from unstructured sources. Online scraping, also known as screen scraping, web harvesting, and web data extraction, allows you to retrieve particular data from websites depending on predefined parameters.

6. **Data Streams**

Data streams are another popular method for collecting continuous streams of data from sources such as instruments, IoT devices and apps, GPS data from cars, computer programmes, websites, and social media posts.

---
## Data Mining Tools

1. **Spreadsheets**

Basic data mining operations are usually performed using spreadsheets such as Microsoft Excel and Google Sheets.

2. **R**

R is a popular data mining tool. R is a free and open-source programming language created by Bell Laboratories (formerly AT&T, now Lucent Technologies)

3. **Python**

Python is a popular programming language that allows programmers to work swiftly and efficiently. Python is highly sought for Machine Learning Algorithms. Many Python feature libraries, such as NumPy, pandas, SciPy, Scikit-Learn, Matplotlib, Tensorflow, and Keras, make machine learning tasks relatively simple to implement.

4. **Statistical Analysis System**

SAS Enterprise Miner is the company’s primary data mining product. Its main goal is to simplify the data mining process by providing a diverse range of analytic skills to assist business users with predictive analysis, which leads to better and more efficient decision making. SAS Enterprise Miner scans through massive volumes of data for patterns, partnerships, particular relationships, or trends that might otherwise be missed.


# Data Cleaning in ML

**Outliers,Duplicate and Missing Value**
Data cleaning is a step in machine learning (ML) which involves identifying and removing any missing, duplicate or irrelevant data.

- Raw data (log file, transactions, audio /video recordings, etc) is often noisy, incomplete and inconsistent which can negatively impact the accuracy of model.
- The goal of data cleaning is to ensure that the data is **accurate, consistent and free of errors** 
- Clean datasets also important in (Exploratory Data Analysis) which enhances the interpretability of data so that the right actions can be taken based on insights.

## How to Perform Data Cleaning

The process begins by identifying issues like missing values, duplicates and outliers. Performing data cleaning involves a systematic process to identify and remove errors in a dataset. The following steps are essential to perform data cleaning:



1. **Remove Unwanted Observations:** Eliminate duplicates, irrelevant entries or redundant data that add noise.
2. **Fix Structural Errors:** Standardize data formats and variable types for consistency.
3. Manage Outliers: Detect and handle extreme values that can skew results, either by removal or transformation.
4. Handle Missing Data: Address gaps using imputation, deletion or advanced techniques to maintain accuracy and integrity.
