### Summary of PyTorch Deep Learning Course Content

This comprehensive course, taught by Daniel Bourke, covers foundational to advanced topics in¬†**machine learning (ML) and deep learning (DL) using PyTorch**, a leading Python framework for building and training neural networks. The course emphasizes hands-on coding, practical experimentation, and understanding core concepts through writing and running PyTorch code.

---

PyTorch is a deep learning library built on Python. It provides GPU acceleration, dynamic computation graphs and an intuitive interface for deep learning researchers and developers. PyTorch follows a "define-by-run" approach meaning that its computational graphs are constructed on the fly allowing for better debugging and model customisation.

### Key Features of PyTorch

- It uses dynamic graphs allowing flexibility in model execution and debugging.
- It provides an **automatic differentiation** engine that simplifies gradient computation for deep learning.
- It supports CUDA allowing computations to be performed efficiently on GPUs.
![[Pasted image 20251229114917.png]]
### Key Concepts and Workflow

- **Machine Learning vs Deep Learning**
    
    - ML: Algorithms that find patterns in numerical representations of data (images, text, audio, etc.).
    - DL: A subset of ML using neural networks with many layers (deep architectures).
    - Artificial Intelligence (AI) is the broadest field encompassing ML and DL.
    **PyTorch** dynamically creates a computational graph that tracks operations and gradients for backpropagation.
    ![[Pasted image 20251229115454.png]]
    In PyTorch, neural networks are built using the¬†**torch.nn** module where:

- ***nn.Linear(in_features, out_features)***¬†defines a fully connected (dense) layer.
- Activation functions like¬†***torch.relu,¬†torch.sigmoid¬†or¬†torch.softmax***¬†are applied between layers.
- ***forward()***¬†method defines how data moves through the network.

To build a neural network in PyTorch, we create a class that inherits from¬†****torch.nn.Module****¬†and defines its layers and forward pass.
- **Traditional Programming vs ML Paradigm**
    
    - Traditional: Write explicit rules to transform inputs into outputs.
    - ML: Provide inputs and outputs; the algorithm learns the mapping rules automatically (supervised learning).
- **Data Representation with Tensors**
    
    - Tensors are multi-dimensional arrays (scalars, vectors, matrices, higher dimensions).
    - Images, text, audio, and other data are encoded as tensors before input to models.
    - PyTorch‚Äôs fundamental data type is¬†`torch.Tensor`.
    - Tensor attributes: shape, dtype (float32, int64, etc.), and device (CPU or GPU).
    - GPU acceleration via CUDA is critical for efficient DL training.
- **Neural Networks and Layers**
    
    - Models subclass¬†`torch.nn.Module`¬†and define a¬†`forward`¬†method for computation.
    - Common layers: Fully Connected (Linear), Convolutional (Conv2d), Pooling (MaxPool2d), Activations (ReLU, Sigmoid, Softmax).
    - Nonlinearity (activation functions) is essential to model complex, nonlinear data patterns.
- **Learning Paradigms**
    
    - Supervised, unsupervised, self-supervised, transfer learning, and reinforcement learning (not covered in depth).
    - Transfer learning: Using pretrained models and fine-tuning them on new tasks.
- **Training Workflow**
    
    - Prepare data (load, transform, batch).
    - Build model (define layers and forward pass).
    - Define loss function and optimizer (e.g., CrossEntropyLoss for classification, SGD or Adam optimizer).
    - Create training loop: forward pass, loss calculation, backpropagation (`loss.backward()`), optimizer step.
    - Create evaluation/testing loop: model in eval mode, inference mode context, forward pass, calculate metrics.
    - Track metrics (loss, accuracy) and training time.
    - Save and load trained models (`torch.save`¬†and¬†`load_state_dict`).
    - Visualization of predictions and performance (loss curves, accuracy curves, confusion matrices).

---

### Data Handling and Custom Datasets

- **Built-in and Custom Data Sets**
    
    - PyTorch offers domain-specific libraries:¬†`torchvision`¬†(vision),¬†`torchaudio`¬†(audio),¬†`torchtext`¬†(text),¬†`torchrec`¬†(recommendation).
    - Use prebuilt datasets and loaders when available (e.g.,¬†`torchvision.datasets.FashionMNIST`).
    - When no prebuilt loader exists, create custom dataset classes by subclassing¬†`torch.utils.data.Dataset`, implementing¬†`__len__`¬†and¬†`__getitem__`.
    - Use¬†`torch.utils.data.DataLoader`¬†to batch and shuffle datasets for efficient training.
- **Image Transformations and Augmentation**
    
    - Use¬†`torchvision.transforms`¬†to resize, crop, normalize, flip, and augment images.
    - Data augmentation increases data diversity artificially (e.g., random flips, rotations, color jitters) to improve model robustness and reduce overfitting.
    - **TrivialAugmentWide**¬†is a recent augmentation method used in state-of-the-art PyTorch models.

---

### Models and Architectures Covered

- **Linear Models**
    
    - Simple feedforward neural networks with fully connected layers for regression and classification.
    - Input tensors are flattened before feeding to linear layers.
- **Classification Models**
    
    - Binary classification: one output with sigmoid activation + binary cross entropy loss.
    - Multi-class classification: multiple outputs with softmax activation + cross entropy loss.
    - Accuracy, precision, recall, F1-score, confusion matrix are key metrics.
- **Convolutional Neural Networks (CNNs)**
    
    - CNN architecture (e.g., Tiny VGG from CNN Explainer) with convolutional layers, ReLU activations, max pooling, flattening, and fully connected output layer.
    - CNN layers convolve kernels over images to extract spatial features.
    - Max pooling reduces spatial dimensions by taking max values in windows (e.g., 2x2).
    - Models trained with cross entropy loss and optimized with Adam or SGD.

---

### Evaluation and Experimentation

- **Training and Testing Loops**
    
    - Functionized loops for reusability and clarity.
    - Use of progress bar (`tqdm`) for monitoring epochs.
    - Training step includes forward pass, loss calculation, backpropagation, optimizer update.
    - Testing step includes model evaluation mode, inference mode, forward pass, loss and accuracy calculation.
- **Loss Curves and Metrics Visualization**
    
    - Plot training and test loss curves to detect underfitting, overfitting, or ideal training.
    - Plot accuracy curves alongside loss curves.
    - Use confusion matrices to analyze misclassifications.
    - Experiment by varying layers, hidden units, data augmentation, learning rates, and number of epochs.
- **Reproducibility and Debugging**
    
    - Set random seeds for reproducibility (`torch.manual_seed`,¬†`torch.cuda.manual_seed`).
    - Common errors: data type mismatches, shape mismatches, device mismatches.
    - Use¬†`torchinfo`¬†package to summarize model architecture and tensor shapes.


---

### Timeline Table: PyTorch Deep Learning Course Flow (Core Topics)

|Section|Topics Covered|
|---|---|
|0: Fundamentals|Machine learning vs deep learning, tensors, PyTorch intro|
|1: PyTorch Workflow|Data preparation, model building, training loop, saving/loading models|
|2: Neural Network Classification|Binary/multi-class classification, training/testing loops, evaluation metrics|
|3: Computer Vision & CNNs|Torchvision data sets, CNN architecture, training CNNs, data augmentation|
|4: Custom Data Sets|Creating and using custom PyTorch datasets and loaders|
|5: Data Augmentation|Applying transforms, data augmentation techniques|
|6: Transfer Learning|Using pretrained models for new tasks (covered later)|
|7: Model Experiment Tracking|Monitoring and comparing experiments (covered later)|
|8: Model Deployment|Exporting models for production (covered later)|

### Core Concepts Summary

- **Tensors**¬†represent data in multi-dimensional arrays; fundamental for PyTorch.
- **Neural Networks**¬†combine linear layers and nonlinear activation functions to model complex data.
- **Training Loop**¬†involves forward pass, loss calculation, backpropagation, and optimizer step.
- **Data Handling**: Load data using domain-specific datasets or custom classes, transform and batch with DataLoader.
- **Evaluation**¬†through metrics (accuracy, confusion matrix) and visualization (loss curves, prediction plots).
- **Device Agnostic Code**¬†ensures seamless training on CPU or GPU.
- **Data Augmentation**¬†enhances training data diversity to improve generalization.
- **Reproducibility**¬†managed through manual seeds and careful device/dtype handling.

---

### Keywords

- PyTorch, Deep Learning, Machine Learning, Neural Networks, Tensors, GPU, CUDA, DataLoader, Dataset, Image Classification, Convolutional Neural Network (CNN), Activation Functions, ReLU, Sigmoid, Softmax, Loss Function, Cross Entropy, Binary Cross Entropy (BCE), Optimizer, SGD, Adam, Data Augmentation, TrivialAugment, Transfer Learning, Model Saving, Model Loading, Accuracy, Confusion Matrix, Visualization, Gradient Descent, Backpropagation, Device Agnostic, Custom Dataset, Torchvision.

---

### Key Insights

- PyTorch is the most popular deep learning framework in research with 58% usage in state-of-the-art papers.
- Deep learning models start with random parameters and improve them via gradient descent and backpropagation.
- Nonlinear activation functions like ReLU are essential for modeling complex, nonlinear data like images.
- Data augmentation (e.g., TrivialAugment) is a powerful technique to improve model generalization.
- Custom datasets in PyTorch require subclassing¬†`torch.utils.data.Dataset`¬†and implementing key methods (`__len__`,¬†`__getitem__`).
- Device mismatches, shape mismatches, and data type mismatches are the most common errors in PyTorch workflows and require careful handling.
- Visualization and monitoring via plots, confusion matrices, and metrics are critical for evaluating model performance and guiding experimentation.

---

### FAQ Highlights

- **Why use PyTorch?**  
    Popular for research, GPU acceleration, flexible API, strong community, and extensive ecosystem.
    
- **What is a tensor?**  
    A multi-dimensional array representing data; fundamental input to PyTorch models.
    
- **Why are nonlinearities important?**  
    They allow neural networks to model complex, nonlinear relationships in data.
    
- **What is data augmentation?**  
    Artificially increasing dataset diversity to improve model generalization.
    
- **How to handle custom datasets?**  
    Subclass¬†`torch.utils.data.Dataset`, implement¬†`__len__`¬†and¬†`__getitem__`, and use DataLoader for batching.
    
- **How to ensure reproducibility?**  
    Set random seeds (`torch.manual_seed`,¬†`torch.cuda.manual_seed`), and maintain consistent device and dtype usage.
    
- **How to evaluate classification models?**  
    Use accuracy, precision, recall, F1-score, confusion matrix, and visualization of predictions.

Here is a **clear, exam-ready and practical explanation** of the **difference between Traditional Algorithms and Machine Learning Algorithms**, explained step-by-step and also useful for **interviews**.

---

### üîπ Traditional Algorithm

A **traditional algorithm** follows **explicit rules written by a programmer** to solve a problem.
> **Input ‚Üí Fixed Rules ‚Üí Output**
---
### üîπ Machine Learning Algorithm
A **machine learning algorithm** learns **patterns from data** and makes decisions **without being explicitly programmed** for every rule.
> **Data + Output ‚Üí Learning ‚Üí Model ‚Üí Prediction**
---
##  Core Differences (Table Format)

| Feature              | Traditional Algorithm            | Machine Learning Algorithm         |
| -------------------- | -------------------------------- | ---------------------------------- |
| Rule Creation        | Human-defined rules              | Learned from data                  |
| Adaptability         | Cannot adapt automatically       | Improves with more data            |
| Data Dependency      | Low                              | Very high                          |
| Accuracy Improvement | Requires manual updates          | Improves through training          |
| Handling Complexity  | Poor for complex patterns        | Excellent for complex patterns     |
| Example Tasks        | Sorting, searching, calculations | Image recognition, NLP, prediction |
| Decision Logic       | If-else, loops                   | Statistical models & optimization  |
| Maintenance          | Manual rule changes              | Retraining model                   |

---
##  Working Principle (Simple)
### üîπ Traditional Algorithm

```text
Programmer writes rules
‚Üì
Computer follows rules
‚Üì
Output produced
```

Example:

```python
if temperature > 38:
    print("Fever")
```

---
### üîπ Machine Learning Algorithm

```text
Data + Labels
‚Üì
Algorithm learns patterns
‚Üì
Model created
‚Üì
Model predicts new data
```

Example:

- Input: Thousands of medical records
    
- Output: Disease prediction model
---

## 4Ô∏è‚É£ Real-World Examples

### üßÆ Traditional Algorithm Examples

- Calculator
- Binary Search
- Sorting algorithms (Bubble, Quick sort)
- Rule-based systems
- Traffic light timers
---
### ü§ñ Machine Learning Examples

- Face recognition
- Recommendation systems (Netflix, YouTube)
- Spam email detection
- Voice assistants
- Fraud detection
---
### üîπ Machine Learning Approach

- Learns patterns from thousands of emails
- Detects spam even with new words  
    ‚úî Adaptive and scalable

---
 Performance Comparison

|Aspect|Traditional|Machine Learning|
|---|---|---|
|Simple Problems|Excellent|Overkill|
|Complex Patterns|Poor|Excellent|
|Big Data|Inefficient|Highly efficient|
|Changing Environment|Fails|Learns & adapts|


![[Pasted image 20251222122019.png]]

# Difference Between **AI, ML, and DL**

- **Artificial Intelligence (AI)**  
    ‚Üí Making machines **act like humans** (thinking, reasoning, decision-making)
- **Machine Learning (ML)**  
    ‚Üí A subset of AI where machines **learn from data**
- **Deep Learning (DL)**  
    ‚Üí A subset of ML that uses **neural networks with many layers**
üìå **Hierarchy (VERY IMPORTANT):**

`Artificial Intelligence (AI)  ‚îî‚îÄ‚îÄ Machine Learning (ML)       ‚îî‚îÄ‚îÄ Deep Learning (DL)`

---
## 2Ô∏è‚É£ Core Differences (Table ‚Äì Interview Friendly)


| Feature             | AI                    | ML                 | DL                                  |
| ------------------- | --------------------- | ------------------ | ----------------------------------- |
| Definition          | Broad concept         | Learning from data | Learning using deep neural networks |
| Dependency          | Logic + rules + data  | Data-driven        | Large data + GPUs                   |
| Human Intervention  | High                  | Medium             | Very low                            |
| Feature Engineering | Manual                | Manual             | Automatic                           |
| Data Requirement    | Low to high           | Medium             | Very high                           |
| Computation         | Low                   | Medium             | Very high                           |
| Example             | Chess-playing program | Spam detection     | Face recognition                    |
### **Artificial Intelligence (AI)**

**Artificial Intelligence** is the broad field of creating machines that can **simulate human intelligence**, such as thinking, reasoning, learning, and decision-making.

---
### **Machine Learning (ML)**

**Machine Learning** is a subset of AI that enables systems to **learn from data and improve performance without being explicitly programmed**.

---
### **Deep Learning (DL)**

**Deep Learning** is a subset of Machine Learning that uses **multi-layered neural networks** to automatically learn complex patterns from large amounts of data.

---
# A Deep Dive into Deep Learning

Deep learning (DL) is a subset of machine learning (ML) that uses artificial neural networks with multiple layers (hence "deep") to model complex patterns in data. It's inspired by the human brain's structure but operates on mathematical principles like backpropagation and gradient descent. 

#### Quick Fundamentals Recap (Build Your Base)

- **Core Idea**: DL models learn hierarchical representations. Shallow layers detect basic features (e.g., edges in images), while deeper ones capture abstract concepts (e.g., faces or emotions).
- **Key Components**:
    - **Neurons and Layers**: Basic units (perceptrons) in input, hidden, and output layers. Activation functions (e.g., ReLU, Sigmoid) introduce non-linearity.
    - **Architectures**:
        - Feedforward Neural Networks (FNN): Simple, for basic prediction.
        - Convolutional Neural Networks (CNNs): Great for grid-like data (images/videos).
        - Recurrent Neural Networks (RNNs) and LSTMs/GRUs: Handle sequences (time series, text).
        - Transformers: Modern powerhouse (e.g., BERT, GPT) using attention mechanisms for parallel processing.
    - **Training**: Uses large datasets, optimization (e.g., Adam optimizer), and loss functions (e.g., cross-entropy for classification).
- **Why "Deep"?** More layers allow for end-to-end learning without manual feature engineering, unlike traditional ML (e.g., SVM or decision trees).
- **Interview Tip**: Be ready for: "Explain overfitting in DL." (Answer: When a model memorizes training data but fails on new data; mitigate with dropout, regularization, or data augmentation.)

#### What Deep Learning is Good For (Strengths and Applications)

DL shines in scenarios with **abundant data**, **complex patterns**, and **high-dimensional inputs** where human-like perception is needed. It's revolutionized fields by automating feature extraction and achieving superhuman performance in specific tasks. Here's a breakdown:

1. **Computer Vision (Image/Video Processing)**:
    - **Why Good?** CNNs excel at spatial hierarchies‚Äîdetecting edges, shapes, and objects invariantly (e.g., regardless of rotation or lighting).
    - **Examples**:
        - Object detection/recognition: Used in self-driving cars (e.g., Tesla's Autopilot via YOLO or Faster R-CNN) to identify pedestrians, signs, and lanes.
        - Medical imaging: Diagnosing diseases from X-rays/MRIs (e.g., Google's DeepMind detecting diabetic retinopathy with 99% accuracy).
        - Facial recognition: Powering security systems and social media tagging.
    - **Real-World Impact**: DL models like ResNet or EfficientNet have won ImageNet competitions, reducing error rates from 28% (2010) to under 3% today.
2. **Natural Language Processing (NLP)**:
    - **Why Good?** Transformers handle context and long-range dependencies better than rule-based systems.
    - **Examples**:
        - Machine translation: Google Translate uses seq2seq models for near-real-time accuracy across 100+ languages.
        - Sentiment analysis/Chatbots: Powering customer service (e.g., GPT-based assistants like me!).
        - Text generation/Summarization: Tools like BERT for search engines or legal document review.
    - **Key Insight**: Pre-trained models (e.g., via transfer learning) allow fine-tuning on smaller datasets, making DL efficient for NLP.
3. **Speech Recognition and Audio Processing**:
    - **Why Good?** RNNs/LSTMs model temporal sequences, capturing phonemes and accents.
    - **Examples**: Siri/Alexa for voice commands; noise cancellation in Zoom; music recommendation on Spotify via audio embeddings.
4. **Recommendation Systems**:
    - **Why Good?** Handles user-item interactions at scale with embeddings (e.g., collaborative filtering).
    - **Examples**: Netflix's movie suggestions (80% of views from DL recs); Amazon's product personalization.
5. **Generative Tasks**:
    - **Why Good?** Models like GANs (Generative Adversarial Networks) or VAEs create new data.
    - **Examples**: Deepfakes, AI art (DALL-E), drug discovery (generating molecular structures).
6. **Reinforcement Learning (Combined with DL)**:
    - **Why Good?** Deep RL (e.g., DQN) learns policies from high-dimensional states.
    - **Examples**: AlphaGo beating humans at Go; robotics for warehouse automation.

**General Strengths**:

- **Scalability**: Performance improves with more data/compute (unlike traditional ML, which plateaus).
- **End-to-End Learning**: No need for domain experts to hand-craft features.
- **Transfer Learning**: Pre-trained models (e.g., from Hugging Face) speed up development.

**Interview Question to Prep**: "When would you choose DL over classical ML?" (Answer: For unstructured data like images/text where patterns are non-linear and data is plentiful; otherwise, stick to simpler models for efficiency.)

#### Where Deep Learning is Not Good (Limitations and Alternatives)
![[Pasted image 20251222140101.png]]
DL is good for Unstructured type of data.
DL isn't a silver bullet‚Äîit's resource-intensive and has blind spots. Interviewers love asking about trade-offs to gauge your critical thinking. Key areas where it's suboptimal:
![[Pasted image 20251222143354.png]]
1. **Small or Low-Quality Datasets**:
    - **Why Not?** DL requires massive data (e.g., millions of samples) to generalize; with little data, it overfits easily.
    - **Examples**: Rare disease prediction (limited cases) or niche industrial sensors.
    - **Alternatives**: Traditional ML (e.g., Random Forests) or data augmentation techniques. Use few-shot learning (e.g., in GPT-3) as a workaround.
2. **Interpretability and Explainability**:
    - **Why Not?** "Black-box" nature‚Äîhard to trace why a decision was made (e.g., why a loan was denied).
    - **Examples**: High-stakes fields like healthcare or finance, where regulations (e.g., GDPR) require transparency.
    - **Alternatives**: Rule-based systems or interpretable ML (e.g., decision trees, LIME/SHAP for post-hoc explanations).
3. **High Computational Cost**:
    - **Why Not?** Training needs GPUs/TPUs and days/weeks of compute; inference can be slow/energy-hungry.
    - **Examples**: Edge devices (e.g., IoT sensors) or real-time apps with latency constraints.
    - **Alternatives**: Lightweight models (e.g., MobileNet) or classical algorithms. Quantization/pruning can help mitigate.
4. **Lack of Common Sense or Causal Reasoning**:
    - **Why Not?** DL is correlation-based, not causation-aware; it memorizes patterns but doesn't "understand" like humans.
    - **Examples**: Ethical AI dilemmas (bias amplification from skewed data, e.g., facial recognition failing on diverse ethnicities) or adversarial attacks (tiny input changes fool models).
    - **Alternatives**: Hybrid systems with symbolic AI or causal ML frameworks (e.g., Pearl's do-calculus).
5. **Tabular/Structured Data**:
    - **Why Not?** Simpler models often outperform DL here due to lower complexity.
    - **Examples**: Financial forecasting with spreadsheets or database queries.
    - **Alternatives**: Gradient Boosting (e.g., XGBoost) or linear regression‚Äîfaster and more accurate for structured tasks.
6. **Energy and Environmental Concerns**:
    - **Why Not?** Training large models (e.g., GPT-4) emits CO2 equivalent to 5 cars' lifetime emissions.
    - **Examples**: Sustainable AI initiatives push for greener alternatives.

**General Weaknesses**:

- **Data Dependency**: Biased data leads to biased models (e.g., gender bias in hiring tools).
- **Brittleness**: Fails on out-of-distribution data (e.g., a self-driving car in unusual weather).
- **Ethical Risks**: Potential for misuse in surveillance or deepfakes.

**Interview Question to Prep**: "What are DL's limitations?" (Answer: Summarize above, then pivot: "To address, we use techniques like federated learning for privacy or ensemble methods for robustness.")

#### Interview Prep Tips

- **Practice Scenarios**: Role-play: "Design a DL system for autonomous drones." (Discuss architecture, data needs, ethics.)
- **Stay Current**: Mention trends like multimodal models (e.g., CLIP for vision+text) or efficient DL (e.g., TinyML).
- **Resources**: Review "Deep Learning" by Goodfellow et al.; practice on Kaggle datasets; use tools from our last chat (e.g., Interviews by AI for mocks).
- **Mindset**: Show enthusiasm but balance with realism‚Äîinterviewers want problem-solvers, not hype-machines.

This should make you knowledge-rich and confident. If you share the job role (e.g., data scientist), I can tailor examples further!

---
---
---
# **Machine Learning algorithms for Structured and Unstructured data**, with **examples** and **why they are used**.
## üîπ Structured vs Unstructured Data (Quick Recall)

### ‚úÖ **Structured Data**

- Tabular form (rows & columns)
    
- Numbers, categories
    
- Example: Excel, SQL tables  
    üëâ _Age, Salary, Marks, Sales_
    

### ‚ùå **Unstructured Data**

- No fixed format
    
- Text, images, audio, video
    
- Example: emails, photos, voice
    

---

## üìä ML Algorithms for **Structured Data**

### üîπ Best Used: **Traditional ML Algorithms**

| Algorithm                             | Used For                    | Example               |
| ------------------------------------- | --------------------------- | --------------------- |
| Linear Regression                     | Prediction                  | House price           |
| Logistic Regression                   | Classification              | Spam / Not spam       |
| Decision Tree                         | Classification & Regression | Loan approval         |
| Random Forest                         | High accuracy               | Fraud detection       |
| Gradient Boosting (XGBoost, LightGBM) | Best performance            | Credit scoring        |
| KNN                                   | Pattern similarity          | Customer segmentation |
| Naive Bayes                           | Probability-based           | Email spam            |
| SVM                                   | High-dimensional data       | Medical diagnosis     |

üìå **Why these work well?**

- Structured data has **clear features**
- Manual feature engineering works well
- Less data required

---

# üß† ML / DL Algorithms for **Unstructured Data**

### üîπ Best Used: **Deep Learning Algorithms**

## üìù Text Data (NLP)

| Algorithm                    | Example               |
| ---------------------------- | --------------------- |
| Naive Bayes                  | Spam filtering        |
| Logistic Regression (TF-IDF) | Sentiment analysis    |
| RNN                          | Language modeling     |
| LSTM / GRU                   | Text generation       |
| Transformers (BERT, GPT)     | Chatbots, translation |

---

## üñº Image Data (Computer Vision)

|Algorithm|Example|
|---|---|
|CNN|Image classification|
|ResNet|Face recognition|
|YOLO|Object detection|
|U-Net|Medical imaging|

---

## üîä Audio / Speech Data

|Algorithm|Example|
|---|---|
|CNN|Audio classification|
|RNN / LSTM|Speech recognition|
|Transformers|Voice assistants|

üìå **Why Deep Learning?**

- Automatically extracts features
- Handles raw text, pixels, audio
- High accuracy for complex patterns

---

# ‚öñÔ∏è Key Comparison (Interview Favorite)

|Feature|Structured Data|Unstructured Data|
|---|---|---|
|Data Format|Table|Text, image, audio|
|Best Algorithms|Tree-based, Regression|Neural networks|
|Feature Engineering|Manual|Automatic|
|Data Size Needed|Small-medium|Large|
|Accuracy|High|Very high (with DL)|

---

# üéØ Rule of Thumb (Very Important)

> **If data is tabular ‚Üí use ML (XGBoost, Random Forest)**  
> **If data is text/image/audio ‚Üí use DL (CNN, RNN, Transformers)**

---

# üß™ Hybrid Case (Real World)

Example: **E-commerce Fraud Detection**

- Structured: price, user age ‚Üí Random Forest
    
- Unstructured: reviews text ‚Üí NLP model  
    ‚úî Combine both for best results
---
---
---
### Neural Networks Explained  

#### 1. What is a Neural Network? 
A neural network is a computational model inspired by the human brain that learns to map inputs to outputs by adjusting weighted connections between simple processing units called neurons, using massive amounts of data and gradient-based optimization.

#### 2. Biological Inspiration vs Reality
- Biological neuron: Dendrites ‚Üí cell body ‚Üí axon ‚Üí synapses
- Artificial neuron: Inputs ‚Üí weighted sum ‚Üí activation function ‚Üí output

We kept the idea of many interconnected units, but everything else is pure math.

#### 3. The Basic Building Block: One Artificial Neuron (Perceptron)

```
Input: x‚ÇÅ, x‚ÇÇ, ..., x‚Çô
Weights: w‚ÇÅ, w‚ÇÇ, ..., w‚Çô
Bias: b

z = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô + b
output = activation(z)
```

Common activation functions (know these!):  

| Activation   | Formula                          | Use Case                          |
|--------------|----------------------------------|-----------------------------------|
| Sigmoid      | 1/(1+e‚Åª·∂ª)                        | Old binary classification        |
| Tanh         | (e·∂ª - e‚Åª·∂ª)/(e·∂ª + e‚Åª·∂ª)            | Hidden layers (zero-centered)     |
| ReLU         | max(0, z)                        | Default today (fast, avoids vanishing gradient) |
| Leaky ReLU   | max(0.01z, z)                    | Fixes "dying ReLU" problem        |
| GELU / SwiGLU| Used in BERT, GPT, LLaMA         | Smooth ReLU variants              |

#### 4. Types of Neural Networks (Interview Cheat Sheet)

| Type                       | Architecture                 | Best For                                        | Famous Examples            |
| -------------------------- | ---------------------------- | ----------------------------------------------- | -------------------------- |
| Feedforward NN (MLP)       | Fully connected layers       | Tabular data, simple regression/classification  | Basic credit scoring       |
| Convolutional NN (CNN)     | Convolutions + pooling       | Images, videos, spatial data                    | ResNet, EfficientNet, YOLO |
| Recurrent NN (RNN)         | Loops, hidden state          | Sequences (text, time series) ‚Äì mostly obsolete | Early machine translation  |
| LSTM / GRU                 | Gated memory cells           | Long sequences (before Transformers)            | Old speech recognition     |
| Transformer                | Self-attention + feedforward | Everything today (text, vision, audio, code)    | GPT, BERT, LLaMA, ViT      |
| Graph Neural Network (GNN) | Message passing on graphs    | Molecules, social networks, recommendation      | GraphSAGE, GATGNN          |
| Spiking Neural Network     | Event-based spikes           | Neuromorphic hardware, ultra-low power          | Emerging (Intel Loihi)     |

![[Pasted image 20251222152822.png]]
#### 5. How Neural Networks Actually Learn (Backpropagation ‚Äì Must Know)

1. Forward pass ‚Üí compute prediction and loss  
   Loss examples: MSE (regression), Cross-Entropy (classification)

2. Backward pass ‚Üí compute gradients ‚àÇLoss/‚àÇw for every weight using chain rule

3. Update weights  
   ```
   w = w - Œ∑ √ó gradient
   Œ∑ = learning rate
   ```

Modern optimizers (better than plain SGD):
- Momentum, RMSprop, Adam (default today), AdamW (for Transformers), Lion, Sophia

#### 6. Universal Approximation Theorem (Why They Work)
One hidden layer with enough neurons can approximate any continuous function arbitrarily well (Cybenko 1989).  
In practice ‚Üí deep networks (many layers) learn hierarchical features much more efficiently.
![[Pasted image 20251222153038.png]]
#### 7. Evolution Timeline (Good for System Design / History Questions)

| Year | Milestone                            | Impact |
|------|--------------------------------------|--------|
| 1958 | Perceptron (Rosenblatt)              | First trainable neuron |
| 1986 | Backpropagation popularized          | Made multi-layer training practical |
| 1989 | CNNs (LeCun) ‚Äì LeNet for digits      | Birth of modern computer vision |
| 2012 | AlexNet wins ImageNet by huge margin | Deep learning explosion (GPU + ReLU + Dropout) |
| 2014 | Seq2Seq + LSTM                       | Modern machine translation |
| 2017 | Transformer (‚ÄúAttention is All You Need‚Äù) | Replaced RNNs everywhere |
| 2020+ | Foundation models (GPT-3, BERT, etc.)| Pre-train once, fine-tune anywhere |

#### 8. When to Use Neural Networks vs When NOT To

| Use Neural Networks                  | Avoid Neural Networks                     |
|--------------------------------------|-------------------------------------------|
| Images, text, audio, video           | Small tabular datasets (<10k rows)        |
| Unstructured data                    | When interpretability is required (finance, medical) |
| You have lots of data + compute      | Real-time on tiny devices (use TinyML or traditional ML) |
| State-of-the-art performance needed  | When simple baseline (XGBoost) already achieves 99% of performance |

#### 9. Quick Interview Answers You Can Memorize

Q: ‚ÄúExplain neural networks in one minute.‚Äù  
A: ‚ÄúA neural network consists of layers of interconnected neurons. Each neuron computes a weighted sum of inputs, adds a bias, and applies a non-linear activation function. The network learns by adjusting weights via backpropagation to minimize a loss function. Deep networks automatically learn hierarchical features ‚Äì early layers detect edges/textures, deeper layers detect objects/concepts ‚Äì which is why they dominate perception tasks today.‚Äù

Q: ‚ÄúWhy did deep learning suddenly work in 2012?‚Äù  
A: Three things aligned:  
1. Big labeled datasets (ImageNet)  
2. GPUs for fast matrix operations  
3. Architectural tricks (ReLU, dropout, batch norm)

#### Want to go deeper?
Just tell me your target role/level and I‚Äôll give you:
- Whiteboard-style diagram practice questions
- System design using neural networks
- Coding implementations (PyTorch/TensorFlow/JAX)
- Latest 2025 research directions (Mamba, liquid networks, test-time training, etc.)

You‚Äôre ready to crush any ‚ÄúWhat are neural networks?‚Äù question now! üöÄ


### Backpropagation Explained in Full Detail ‚Äì Interview-Ready Version (2025)

This is the exact level of depth expected when a senior ML engineer, Deep Learning researcher, or FAANG interviewer asks: ‚ÄúExplain backpropagation in detail‚Äù or ‚ÄúDerive backpropagation from scratch.‚Äù

#### 1. What Is Backpropagation? (One-Sentence Answer)
Backpropagation is the algorithm that efficiently computes the partial derivatives (gradients) of the loss function with respect to every weight and bias in a neural network by applying the chain rule layer-by-layer from the output back to the input.

It is the reason we can train deep networks with millions or billions of parameters.

#### 2. Intuition First ‚Äì Forward Pass vs Backward Pass

**Forward Pass** (what you already know):
Input ‚Üí Layer 1 ‚Üí Layer 2 ‚Üí ‚Ä¶ ‚Üí Output ‚Üí Loss

**Backward Pass** (backpropagation):
Loss ‚Üí ‚àáOutput ‚Üí ‚àáLayer N ‚Üí ‚àáLayer N-1 ‚Üí ‚Ä¶ ‚Üí ‚àáLayer 1 ‚Üí ‚àáWeights

We go backwards because we know how the loss depends on the final output, and we use the chain rule repeatedly to find how it depends on earlier weights.

#### 3. Mathematical Setup (Notation You Must Know)

Let‚Äôs consider a simple 3-layer network:

```
x (input) ‚Üí [W¬π, b¬π] ‚Üí h¬π ‚Üí [W¬≤, b¬≤] ‚Üí h¬≤ ‚Üí [W¬≥, b¬≥] ‚Üí yÃÇ ‚Üí L(yÃÇ, y)
```

- L = loss (e.g., CrossEntropy, MSE)
- z¬π = W¬πx + b¬π      ‚Üí pre-activation
- h¬π = œÉ(z¬π)          ‚Üí activation (œÉ = ReLU, sigmoid, etc.)
- Same for layer 2 and 3
- Final output yÃÇ = softmax(z¬≥) or just z¬≥ (depending on task)

Goal: Compute ‚àÇL/‚àÇW¬π, ‚àÇL/‚àÇb¬π, ‚àÇL/‚àÇW¬≤, ‚Ä¶ for every parameter.

#### 4. The Four Fundamental Equations of Backpropagation

These four equations are asked in almost every serious interview.

**Equation 1** ‚Äì Gradient of loss w.r.t. final pre-activation:
```
Œ¥¬≥ = ‚àÇL / ‚àÇz¬≥ = ‚àÇL / ‚àÇyÃÇ √ó ‚àÇyÃÇ / ‚àÇz¬≥
```
Example: If L = CrossEntropy + softmax, then Œ¥¬≥ = yÃÇ ‚àí y (one-hot true label)

**Equation 2** ‚Äì Backprop through activation function:
```
Œ¥¬≤ = ‚àÇL / ‚àÇz¬≤ = (W¬≥·µÄ Œ¥¬≥) ‚äô œÉ'(z¬≤)
```
‚äô = element-wise multiplication

**Equation 3** ‚Äì Gradient w.r.t. weights and biases:
```
‚àÇL / ‚àÇW¬≥ = Œ¥¬≥ (h¬≤)·µÄ
‚àÇL / ‚àÇb¬≥ = Œ¥¬≥
‚àÇL / ‚àÇW¬≤ = Œ¥¬≤ (h¬π)·µÄ
...
```

**Equation 4** ‚Äì Continue backwards:
```
Œ¥¬π = (W¬≤·µÄ Œ¥¬≤) ‚äô œÉ'(z¬π)
```

You repeat this pattern all the way to the input.

These Œ¥ terms (Œ¥^L) are called "error signals" or "gradients of loss w.r.t. pre-activations".

#### 5. Full Derivation Example ‚Äì One Hidden Layer (Whiteboard Classic)

Task: Binary classification, 1 hidden layer, sigmoid output.

```
Loss L = -[y log yÃÇ + (1-y) log(1-yÃÇ)]   (binary cross-entropy)
yÃÇ = sigmoid(z¬≤) = œÉ(W¬≤ h + b¬≤)
h = ReLU(z¬π) = ReLU(W¬π x + b¬π)
```

Step-by-step backward:

1. ‚àÇL/‚àÇyÃÇ = -(y/yÃÇ - (1-y)/(1-yÃÇ)) = yÃÇ - y
2. ‚àÇL/‚àÇz¬≤ = ‚àÇL/‚àÇyÃÇ √ó œÉ'(z¬≤) = (yÃÇ - y) √ó yÃÇ(1-yÃÇ)   ‚Üí Œ¥¬≤
3. ‚àÇL/‚àÇW¬≤ = Œ¥¬≤ h·µÄ
4. ‚àÇL/‚àÇh = W¬≤·µÄ Œ¥¬≤
5. ‚àÇL/‚àÇz¬π = ‚àÇL/‚àÇh ‚äô ReLU'(z¬π) = ‚àÇL/‚àÇh ‚äô (z¬π > 0)   ‚Üí Œ¥¬π
6. ‚àÇL/‚àÇW¬π = Œ¥¬π x·µÄ

Done. You now have all gradients.

#### 6. Why It‚Äôs Efficient ‚Äì Computational Graph View

Modern frameworks (PyTorch, JAX, TensorFlow) build a dynamic computation graph during the forward pass and automatically apply the chain rule backward (autograd).

Each operation (add, mul, ReLU, matmul, softmax) knows its local gradient ‚Üí chain rule does the rest.

#### 7. Key Tricks That Make Backprop Work in Deep Networks

| Problem                        | Solution Used in Practice                    |
|--------------------------------|-----------------------------------------------|
| Vanishing gradients           | ReLU, GELU, LayerNorm, Residual connections  |
| Exploding gradients            | Gradient clipping, good initialization       |
| Slow training                  | Batch normalization ‚Üí gone in 2024+, now mostly LayerNorm |
| Numerical stability            | LogSoftmax + NLLLoss instead of Softmax + CrossEntropy |

#### 8. Modern Variants & Optimizations (2024‚Äì2025)

| Technique                  | What It Changes                                      | Used In                     |
|----------------------------|-------------------------------------------------------|-----------------------------|
| Backprop through time (BPTT)| For RNNs ‚Äì unroll sequence                            | Old LSTMs                   |
| Gradient Checkpointing     | Trade compute for memory (recompute activations)      | Training 100B+ models       |
| Adjoint method / Optimal control | Continuous-time version of backprop                | Neural ODEs, Liquid Networks|
| Zero (ZeRO Optimizer)      | Shards gradients, activations, parameters             | DeepSpeed, training Grok-1  |
| FlashAttention backprop    | Fused kernel ‚Äì 3‚Äì5√ó faster backward pass              | LLaMA-3, Gemma, Qwen2       |

#### 9. Interview Answering Template (3‚Äì5 Minutes)

‚ÄúBackpropagation is the chain-rule application from loss to parameters using intermediate error terms Œ¥^l.

We start with Œ¥^L at the output layer: for cross-entropy + softmax, it simplifies to yÃÇ ‚àí y.

Then recursively:
Œ¥^l = (W^{l+1}^T Œ¥^{l+1}) ‚äô œÉ'(z^l)

And the weight gradient is always:
‚àÇL/‚àÇW^l = Œ¥^l (a^{l-1})^T

This gives us O(1) extra cost per layer compared to forward pass, which is why we can train networks with thousands of layers today. Residual connections and good normalization keep gradients well-behaved even at depth 1000+.‚Äù

#### 10. Common Follow-Up Questions & Perfect Answers

Q: ‚ÄúWhy did ReLU solve vanishing gradients?‚Äù  
A: Sigmoid/Tanh squash outputs to [0,1] or [-1,1] ‚Üí gradients ‚â§ 0.25 ‚Üí multiply 100 times ‚Üí near zero. ReLU has gradient 1 when active.

Q: ‚ÄúCan you train a network without backpropagation?‚Äù  
A: Yes ‚Äì target propagation, synthetic gradients, evolutionary strategies, forward-forward (Hinton 2022), equilibrium propagation ‚Äì but they‚Äôre slower or less stable.

Q: ‚ÄúHow does LayerNorm help backprop?‚Äù  
A: Normalizes activations per token ‚Üí stabilizes gradient flow, reduces internal covariate shift, makes training much less sensitive to learning rate.
### Transfer Learning Explained ‚Äì Interview-Ready Depth (2025 Edition)

This is exactly how you should answer ‚ÄúWhat is transfer learning?‚Äù in any ML/AI interview ‚Äî from fresh grad to senior researcher at FAANG, OpenAI, xAI, Anthropic, etc.

#### 1. One-Sentence Definition (Perfect for Interviews)
Transfer learning is the technique of taking a neural network pre-trained on a large, general dataset (usually on a different but related task) and reusing its learned features as a starting point to solve a new, often smaller or different target task ‚Äî dramatically reducing training time and data requirements.

#### 2. Why Transfer Learning Became Essential (The Real Reason)
Training a deep neural network from scratch requires:
- Millions to billions of labeled examples
- Weeks of GPU/TPU time
- Huge engineering effort

Most real-world problems don‚Äôt have that much labeled data.

Transfer learning solves this by leveraging knowledge learned from ‚Äúsource tasks‚Äù (ImageNet, Wikipedia, internet text) and transferring it to ‚Äútarget tasks‚Äù (medical imaging, sentiment analysis on tweets, robot control, etc.).

#### 3. Two Main Types of Transfer Learning (Must Know for Interviews)

| Type                  | What You Reuse                  | When to Use                                      | Classic Examples                          |
|-----------------------|---------------------------------|--------------------------------------------------|-------------------------------------------|
| **Feature Extraction** (Frozen backbone) | Pre-trained model as fixed feature extractor | Very small target dataset (<1k‚Äì10k samples)      | ResNet50 ‚Üí extract features ‚Üí train small classifier on top |
| **Fine-Tuning**        | Pre-trained weights + continue training all/some layers | Medium to large target dataset, want best performance | BERT + one linear layer ‚Üí fine-tune all weights on GLUE tasks |

#### 4. How Transfer Learning Actually Works ‚Äì Step by Step

1. **Pre-training Phase** (done once by big labs/companies)
   - Train huge model on massive dataset
   - Examples:
     - Vision: ImageNet-1k (1.3M images, 1000 classes) ‚Üí ResNet, ViT, ConvNeXt
     - Language: Billions of webpages ‚Üí BERT, RoBERTa, GPT, LLaMA
     - Multimodal: Image-text pairs ‚Üí CLIP, Flamingo, LLaVA

2. **Transfer Phase** (what you do in practice)
   - Take the pre-trained weights
   - Replace or add final layers for your task
   - Two strategies:
     - Freeze lower layers (they learned general features: edges ‚Üí textures ‚Üí objects)
     - Train only top layers first (warm-up), then unfreeze and fine-tune everything at low learning rate

#### 5. Real-World Examples (Drop These in Interviews)

| Domain       | Pre-trained Model         | Target Task                          | Data Needed Without Transfer | With Transfer |
|--------------|---------------------------|--------------------------------------|------------------------------|---------------|
| Medical Imaging | ImageNet ResNet/ViT       | Detect tumors in CT scans            | ~100,000 labeled scans       | <500         |
| NLP           | BERT / RoBERTa            | Sentiment analysis on tweets         | Millions of labeled tweets   | ~5,000        |
| Speech        | wav2vec 2.0 / Whisper     | Transcribe rare language             | 10,000+ hours                | <100 hours    |
| Code          | CodeBERT / StarCoder      | Bug detection                        | Huge labeled code            | Few thousand  |
| Robotics      | CLIP + ImageNet           | Grasp objects never seen before      | Millions of robot trials     | Few hundred   |

#### 6. Modern Transfer Learning Paradigms (2024‚Äì2025)

| Paradigm                | Description                                                    | Best Models 2025                           |
| ----------------------- | -------------------------------------------------------------- | ------------------------------------------ |
| **Foundation Models**   | Giant pre-trained models used for hundreds of tasks            | GPT-4o, LLaMA-3, Gemini 1.5, Grok-2        |
| **Instruction Tuning**  | Fine-tune on instruction-following data ‚Üí zero-shot capability | Alpaca, Vicuna, Phi-3, Mistral-7B-Instruct |
| **In-Context Learning** | No weight updates ‚Äî just give examples in prompt (GPT-3+)      | All decoder-only LLMs                      |
| **Adapters**            | Insert small trainable modules, keep original weights frozen   | LoRA, QLoRA, AdapterHub                    |
| **LoRA / QLoRA**        | Low-Rank Adaptation ‚Äî train <1% of parameters                  | Used in LLaMA fine-tunes, Stable Diffusion |

#### 7. When Transfer Learning Fails (Important for Critical Thinking Questions)

| Scenario                               | Why It Fails                              | Solution                             |
|----------------------------------------|-------------------------------------------|--------------------------------------|
| Domain shift too large                 | ImageNet ‚Üí satellite imagery              | Domain adaptation, self-supervised pre-training on target domain |
| Task is too different                  | Language model ‚Üí play Go                  | Usually not possible ‚Äî need full training |
| Negative transfer                      | Pre-training hurts performance            | Try different source model or train from scratch |

#### 8. Interview Answering 

‚ÄúTransfer learning reuses knowledge from a source task (usually large-scale like ImageNet or web text) to improve performance on a target task with limited data. There are two main approaches: feature extraction, where we freeze the backbone and train only a new head, and fine-tuning, where we update all or most layers at a lower learning rate.

It works because lower layers learn universal features (edges, textures, word embeddings), while higher layers become task-specific. This is why fine-tuning a pre-trained ViT on 100 medical images can outperform a CNN trained from scratch on 50,000.

Today, with foundation models like LLaMA-3 or CLIP, transfer learning has evolved into prompt tuning, in-context learning, and parameter-efficient methods like LoRA ‚Äî enabling powerful models even on consumer GPUs.‚Äù

#### Bonus: Perfect Follow-Up Answers

Q: ‚ÄúWhy does fine-tuning need lower learning rate?‚Äù  
A: The pre-trained weights are already near a good solution. High LR would destroy the useful features.

Q: ‚ÄúLoRA vs full fine-tuning?‚Äù  
A: LoRA adds low-rank matrices (0.1‚Äì1% trainable params), same performance, 10‚Äì100√ó less GPU memory.

Now you can confidently explain transfer learning at any depth ‚Äî from intern to staff engineer level.

---
---
### CUDA ‚Äì What It Actually Is 

**CUDA = Compute Unified Device Architecture**  
NVIDIA‚Äôs parallel computing platform and programming model that lets you use NVIDIA GPUs for general-purpose computing (GPGPU), not just graphics.

In simple terms: CUDA is the reason your neural network trains 10‚Äì100√ó faster than on CPU.

| Concept                  | What It Means in Practice                                      |
|--------------------------|-----------------------------------------------------------------|
| **CUDA Cores**           | The actual tiny processors inside an NVIDIA GPU (e.g., RTX 4090 has 16,384 cores) |
| **Kernel**               | A function that runs in parallel on thousands of GPU threads   |
| **Grid ‚Üí Blocks ‚Üí Threads** | How you organize millions of parallel threads                  |
| **Shared Memory**        | Ultra-fast on-chip memory (like L1 cache) per block            |
| **Global Memory**        | Big but slow DRAM on the GPU card                              |
| **Tensor Cores**         | Special hardware for fast FP16/BF16/FP8 matrix math (used in LLMs) |
| **Streams & Graphs**     | Overlap computation and memory copies (advanced speed tricks) |

#### Real-World Impact (2025 numbers)
| GPU                | CUDA Cores | Tensor Cores | Training GPT-3 175B speed vs CPU |
|--------------------|------------|--------------|----------------------------------|
| A100 (2020)        | 6912       | 432          | ~50√ó faster                      |
| H100 (2023)        | 16896      | 528          | ~120√ó faster                     |
| Blackwell B200 (2025) | ~20k+    | 1000+        | ~200√ó+ faster                    |

#### Everyday Tools That Use CUDA Under the Hood
- PyTorch ‚Üí torch.cuda  
- TensorFlow ‚Üí tf.device('/gpu:0')  
- JAX ‚Üí runs on CUDA via XLA  
- Stable Diffusion, LLaMA.cpp, Ollama, vLLM, FlashAttention  
- Almost every AI paper from 2012‚Äì2025

#### One-Liner Interview Answer
‚ÄúCUDA is NVIDIA‚Äôs proprietary parallel programming interface that exposes thousands of GPU cores for general computation. It‚Äôs the foundation of modern deep learning acceleration ‚Äî PyTorch, TensorFlow, and JAX all compile to CUDA kernels, and newer Tensor Cores give another 4‚Äì10√ó speedup specifically for mixed-precision matrix multiplies used in Transformers.‚Äù

#### Bonus: Quick PyTorch CUDA Check
```python
import torch
print(torch.cuda.is_available())        # True if you have NVIDIA GPU + drivers
print(torch.cuda.get_device_name(0))     # e.g., "NVIDIA RTX 4090"
print(torch.cuda.get_device_capability()) # e.g., (8, 9) = Ada Lovelace
```

‚ÄúCUDA is NVIDIA‚Äôs proprietary parallel programming interface that exposes thousands of GPU cores for general computation. It‚Äôs the foundation of modern deep learning acceleration ‚Äî PyTorch, TensorFlow, and JAX all compile to CUDA kernels, and newer Tensor Cores give another 4‚Äì10√ó speedup specifically for mixed-precision matrix multiplies used in Transformers.

---
![[Pasted image 20251222154809.png]]

|Feature|**GPU**|**TPU**|
|---|---|---|
|Full Form|Graphics Processing Unit|Tensor Processing Unit|
|Purpose|General-purpose parallel computing|AI/ML-specific acceleration|
|Designed By|NVIDIA / AMD|Google|
|Flexibility|High (many workloads)|Low (ML-focused)|
|Best For|ML, DL, graphics, gaming|Large-scale deep learning|
|Optimization|General parallel tasks|Matrix/tensor operations|
|Availability|Widely available|Mostly on Google Cloud|
|Ease of Use|Easier, well-supported|Requires TensorFlow/XLA|
### **What is a Tensor? (Simple & Interview-Ready)**

**A tensor is a multi-dimensional array used to store and process data in machine learning and deep learning.** Any representation of numerical data is called Tensors. 

---
### üìä Tensor by Dimensions

|Dimension|Name|Example|
|---|---|---|
|0D|Scalar|`5`|
|1D|Vector|`[1, 2, 3]`|
|2D|Matrix|`[[1,2],[3,4]]`|
|3D+|Tensor|Images, videos, batches|
![[Pasted image 20251222162227.png]]

![[Pasted image 20251222162503.png]]

https://www.youtube.com/watch?v=V_xro1bcAuA

https://github.com/mrdbourke/pytorch-deep-learning/blob/main/01_pytorch_workflow.ipynb

https://www.geeksforgeeks.org/deep-learning/introduction-deep-learning/

https://www.geeksforgeeks.org/machine-learning/machine-learning/

https://www.khanacademy.org/math/statistics-probability/analyzing-categorical-data/one-categorical-variable/v/identifying-individuals-variables-and-categorical-variables-in-a-data-set